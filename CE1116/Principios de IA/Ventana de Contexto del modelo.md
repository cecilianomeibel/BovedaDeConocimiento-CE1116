---
Fecha de creaci贸n: 2025-09-01 21:59
Fecha de Modificaci贸n: 2025-09-01 21:59
tags: 
Tema:
---


##  Idea/Concepto 
En un modelo de lenguaje de gran tama帽o (LLM) cuando se procesa un texto no se analiza cada palabra de manera individual sino que se tiene en cuenta la relaci贸n de la palabra con las otras palabras de la secuencia, la cantidad de tokens que el modelo puede procesar o considerar es lo que se conoce como ventana de contexto. La ventana de contexto tiene una memoria de trabajo finita, en determinado momento si llega a una cantidad de tokens l铆mite puede perderse el hilo en conversaciones extendidas. El mecanismo de atenci贸n utiliza esto para construir el significado y escala cuadr谩ticamente en el tama帽o.

##  Puntos Claves (Opcional)
- 

##  Connections
- [[Principios de IA]]

##  Personal Insight (Opcional)
- 
## Ь Recursos (Opcional)
- 