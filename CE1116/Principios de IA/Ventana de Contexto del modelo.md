---
Fecha de creación: 2025-09-01 21:59
Fecha de Modificación: 2025-09-01 21:59
tags: 
Tema:
---


## 📚 Idea/Concepto 
En un modelo de lenguaje de gran tamaño (LLM) cuando se procesa un texto no se analiza cada palabra de manera individual sino que se tiene en cuenta la relación de la palabra con las otras palabras de la secuencia, la cantidad de tokens que el modelo puede procesar o considerar es lo que se conoce como ventana de contexto. La ventana de contexto tiene una memoria de trabajo finita, en determinado momento si llega a una cantidad de tokens límite puede perderse el hilo en conversaciones extendidas. El mecanismo de atención utiliza esto para construir el significado y escala cuadráticamente en el tamaño.

## 📌 Puntos Claves (Opcional)
- 

## 🔗 Connections
- [[Principios de IA]]

## 💡 Personal Insight (Opcional)
- 
## 🧾 Recursos (Opcional)
- 